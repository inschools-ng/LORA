{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d768f7e492114bb8b853905d16cde357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_424b5dbab1594b038e2114917e8d154f",
              "IPY_MODEL_9cdea2b7135c4e5fa11edf96fae70504",
              "IPY_MODEL_dbcec891572b418bb6716e5b63bd825e"
            ],
            "layout": "IPY_MODEL_1b6568100c27490fa496342fa937f5c8"
          }
        },
        "424b5dbab1594b038e2114917e8d154f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6daeb63aba0d4c79ba8390e77583bd70",
            "placeholder": "​",
            "style": "IPY_MODEL_399bd6ea7b3f4f1893c24fa5573e0fa0",
            "value": "Map: 100%"
          }
        },
        "9cdea2b7135c4e5fa11edf96fae70504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0799f4aaede41b3bb4e65e9d5f4954a",
            "max": 12000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e2f3e5251874b559cbbca262bbd2268",
            "value": 12000
          }
        },
        "dbcec891572b418bb6716e5b63bd825e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ef758f9ec7491a8eca1ecf7bce3c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_168543a39f2a4d3cb5f93e7855a969da",
            "value": " 12000/12000 [00:01&lt;00:00, 6826.40 examples/s]"
          }
        },
        "1b6568100c27490fa496342fa937f5c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6daeb63aba0d4c79ba8390e77583bd70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "399bd6ea7b3f4f1893c24fa5573e0fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0799f4aaede41b3bb4e65e9d5f4954a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e2f3e5251874b559cbbca262bbd2268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36ef758f9ec7491a8eca1ecf7bce3c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "168543a39f2a4d3cb5f93e7855a969da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets evaluate peft accelerate pandas -U -q\n",
        "print(\"Libraries installed in fresh environment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytti7MU2KJ-w",
        "outputId": "815e95b5-2e93-4ff7-9d66-2ce3e5d5223e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mLibraries installed in fresh environment.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Step 1: Imports\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    get_linear_schedule_with_warmup # Explicitly import scheduler if customizing\n",
        ")\n",
        "from peft import LoraConfig, TaskType, get_peft_model, PeftModel # Ensure PeftModel is imported for loading\n",
        "from google.colab import drive\n",
        "#from transformers.optimization import AdamW #  AdamW from transformers.optimization does not seem to work also"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "yWkxfm6XLCQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_MOUNTED = True\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    print(\"Proceeding without Google Drive. Models and results will not be saved persistently.\")\n",
        "    DRIVE_MOUNTED = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yvytP60Khaw",
        "outputId": "6444b368-a12d-451b-b4b8-249ed6fa0e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Configuration (Reduced for faster testing)\n",
        "MODEL_CHECKPOINT = \"roberta-base\"\n",
        "MAX_LENGTH = 256\n",
        "BATCH_SIZE = 16 # Adjust based on GPU memory\n",
        "NUM_LABELS = 4 # AGNEWS\n",
        "\n",
        "# --- Configuration for Hyperparameter Sweep (Reduced for speed) ---\n",
        "# Expand these lists to explore more configurations\n",
        "sweep_ranks = [4, 8] # Example: Test rank 4\n",
        "sweep_alphas = [8, 16] # Example: Test alpha 8 (often r*2)\n",
        "sweep_target_modules = [[\"query\", \"value\"], [\"query\", \"key\", \"value\"]] # Example: Test targeting query and value\n",
        "sweep_learning_rates = [5e-5, 1e-4] # Example: Test one learning rate\n",
        "sweep_lora_dropout = [0.1]\n",
        "sweep_epochs = [1, 3] # Reduced for faster experimentation\n",
        "\n",
        "\n",
        "print(\"--- Hyperparameter Sweep Configuration ---\")\n",
        "print(f\"Ranks: {sweep_ranks}\")\n",
        "print(f\"Alphas: {sweep_alphas}\")\n",
        "print(f\"Target Modules: {sweep_target_modules}\")\n",
        "print(f\"Learning Rates: {sweep_learning_rates}\")\n",
        "print(f\"Dropout Values: {sweep_lora_dropout}\")\n",
        "print(f\"Epochs: {sweep_epochs}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# --- Output Directory on Google Drive ---\n",
        "# Define a base directory on Google Drive IF mounted\n",
        "if DRIVE_MOUNTED:\n",
        "    DRIVE_BASE_DIR = \"/content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project\"\n",
        "    os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "else:\n",
        "    DRIVE_BASE_DIR = \"./AGNEWS_LORA_Project_Local\" # Save locally if Drive not mounted\n",
        "    os.makedirs(DRIVE_BASE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Checkpoints and results will be saved under: {DRIVE_BASE_DIR}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmTSpFYhNrFq",
        "outputId": "a8758937-e1cd-424e-cabd-43d287bb164a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Hyperparameter Sweep Configuration ---\n",
            "Ranks: [4, 8]\n",
            "Alphas: [8, 16]\n",
            "Target Modules: [['query', 'value'], ['query', 'key', 'value']]\n",
            "Learning Rates: [5e-05, 0.0001]\n",
            "Dropout Values: [0.1]\n",
            "Epochs: [1, 3]\n",
            "----------------------------------------\n",
            "Checkpoints and results will be saved under: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load and Prepare Dataset (Using Hugging Face Hub version)\n",
        "print(\"Loading AGNEWS dataset...\")\n",
        "try:\n",
        "    # Attempt to load directly from Hugging Face Hub\n",
        "    agnews_dataset = load_dataset(\"ag_news\")\n",
        "    # Optional: Create a validation split if desired (useful for seeing validation performance during training)\n",
        "    train_splits = agnews_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
        "    prepared_datasets = DatasetDict({\n",
        "         'train': train_splits['train'],\n",
        "         'validation': train_splits['test'], # Use this for eval during training\n",
        "         'test': agnews_dataset['test']      # Keep original test set separate\n",
        "     })\n",
        "    # Simpler: Just use train/test split provided\n",
        "    #prepared_datasets = agnews_dataset\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load dataset from Hugging Face Hub: {e}\")\n",
        "    print(\"Please ensure internet connection is available or provide data files manually.\")\n",
        "    # Add fallback to load from local files (e.g., CSV) if needed, similar to Kaggle example\n",
        "    # For now, exit if dataset loading fails\n",
        "    raise SystemExit(\"Dataset loading failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cgk2f21NuDL",
        "outputId": "d8238ca3-5a92-439b-9940-e96a406e65ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading AGNEWS dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 5: Load Tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA0tagoHNyvu",
        "outputId": "303c4e59-c9d4-4c3f-dbe7-055aa8c7d930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Define Tokenization Function\n",
        "def tokenize_function(examples):\n",
        "    # Handles text column possibly named 'text' or 'Description' etc.\n",
        "    text_column = \"text\" # Default for HF ag_news\n",
        "    if text_column not in examples:\n",
        "        # Attempt common alternatives if 'text' isn't present\n",
        "        possible_cols = [col for col in examples.keys() if isinstance(examples[col][0], str)]\n",
        "        if possible_cols:\n",
        "            text_column = possible_cols[0] # Use the first string column found\n",
        "            print(f\"Auto-detected text column: '{text_column}'\")\n",
        "        else:\n",
        "            raise ValueError(\"Could not automatically detect text column in dataset.\")\n",
        "\n",
        "    return tokenizer(examples[text_column], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n"
      ],
      "metadata": {
        "id": "GDZUr0vmN-pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 7: Apply Tokenization and Formatting\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_datasets = prepared_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# Determine the original text column name to remove it\n",
        "text_col_to_remove = \"text\" # Default\n",
        "if text_col_to_remove not in tokenized_datasets['train'].column_names:\n",
        "    possible_cols = [col for col in tokenized_datasets['train'].features if isinstance(tokenized_datasets['train'][0][col], str)]\n",
        "    if possible_cols:\n",
        "        text_col_to_remove = possible_cols[0]\n",
        "\n",
        "columns_to_remove = [text_col_to_remove] if text_col_to_remove in tokenized_datasets['train'].column_names else []\n",
        "if not columns_to_remove:\n",
        "     print(f\"Warning: Could not find text column '{text_col_to_remove}' to remove after tokenization.\")\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\") # Assumes original label column is 'label'\n",
        "tokenized_datasets.set_format(\"torch\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "d768f7e492114bb8b853905d16cde357",
            "424b5dbab1594b038e2114917e8d154f",
            "9cdea2b7135c4e5fa11edf96fae70504",
            "dbcec891572b418bb6716e5b63bd825e",
            "1b6568100c27490fa496342fa937f5c8",
            "6daeb63aba0d4c79ba8390e77583bd70",
            "399bd6ea7b3f4f1893c24fa5573e0fa0",
            "c0799f4aaede41b3bb4e65e9d5f4954a",
            "4e2f3e5251874b559cbbca262bbd2268",
            "36ef758f9ec7491a8eca1ecf7bce3c6f",
            "168543a39f2a4d3cb5f93e7855a969da"
          ]
        },
        "id": "-eXxZqF3OAzn",
        "outputId": "d9d38451-20d0-461f-b205-db656428aa72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing datasets...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d768f7e492114bb8b853905d16cde357"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 8: Data Collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "nfUmbr_-OGUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 9: Define Compute Metrics Function\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy_metric.compute(predictions=predictions, references=labels)\n"
      ],
      "metadata": {
        "id": "h-oeNkLPOI9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 10: Define Parameter Check Function\n",
        "def check_params(model_to_check, limit=1_000_000):\n",
        "    trainable_params = sum(p.numel() for p in model_to_check.parameters() if p.requires_grad)\n",
        "    print(f\"--> Trainable Parameters: {trainable_params:,}\")\n",
        "    if trainable_params > limit:\n",
        "        print(f\"--> WARNING: Exceeds {limit:,} parameter limit!\")\n",
        "        return False, trainable_params\n",
        "    else:\n",
        "        print(f\"--> Within {limit:,} parameter limit.\")\n",
        "        return True, trainable_params\n"
      ],
      "metadata": {
        "id": "Hl3xQFF8OLsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSkhp4OQnrxS",
        "outputId": "706e2001-e170-4bd0-8d13-f389bfca1b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.51.3\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Hyperparameter Sweep Loop (Corrected & Expanded for Epochs)\n",
        "#############################################################################\n",
        "#                       !!! IMPORTANT NOTE !!!                             #\n",
        "# Ensure the following are defined in previous cells before running this:   #\n",
        "# - Necessary libraries imported (torch, os, gc, transformers, datasets, peft, etc.) #\n",
        "# - Global constants: MODEL_CHECKPOINT, BATCH_SIZE, NUM_LABELS             #\n",
        "# - Sweep lists: sweep_epochs, sweep_ranks, sweep_alphas,                  #\n",
        "#   sweep_target_modules, sweep_learning_rates, sweep_lora_dropout        #\n",
        "# - DRIVE_BASE_DIR: Path to save outputs on Google Drive                   #\n",
        "# - tokenized_datasets: Processed train/validation/test datasets          #\n",
        "# - tokenizer: Loaded tokenizer                                            #\n",
        "# - data_collator: Initialized DataCollatorWithPadding                    #\n",
        "# - compute_metrics: Defined function for evaluation metrics              #\n",
        "# - check_params: Defined function to check trainable parameters          #\n",
        "# - id2label_map, label2id_map: Derived or defined label mappings        #\n",
        "#############################################################################\n",
        "\n",
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    DataCollatorWithPadding # Ensure imported if not globally\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel # Ensure imported\n",
        "\n",
        "print(\"\\n--- Starting Expanded Hyperparameter Sweep ---\")\n",
        "\n",
        "# Initialize results and details list BEFORE the loops\n",
        "results = {}\n",
        "all_run_details = []\n",
        "\n",
        "# --- Loop through all hyperparameter combinations ---\n",
        "for epochs_val in sweep_epochs: # Iterate through the number of epochs\n",
        "    for r_val in sweep_ranks:\n",
        "        for alpha_val in sweep_alphas:\n",
        "            # Optional: Add logic here to skip non-standard alpha/rank pairs if desired\n",
        "            # e.g., if alpha_val != 2 * r_val:\n",
        "            #    print(f\"Skipping alpha={alpha_val} for r={r_val}\")\n",
        "            #    continue\n",
        "            for targets in sweep_target_modules:\n",
        "                for lr in sweep_learning_rates:\n",
        "                    for dropout_val in sweep_lora_dropout:\n",
        "\n",
        "                        # --- Define unique key and output directory ---\n",
        "                        config_key = f\"r={r_val}_alpha={alpha_val}_targets={'_'.join(targets)}_lr={lr}_dropout={dropout_val}_epochs={epochs_val}\"\n",
        "                        run_output_dir = os.path.join(DRIVE_BASE_DIR, config_key) # Assumes DRIVE_BASE_DIR is defined\n",
        "                        try:\n",
        "                           os.makedirs(run_output_dir, exist_ok=True)\n",
        "                        except OSError as ose:\n",
        "                           print(f\"WARNING: Could not create directory {run_output_dir}: {ose}. Check path/permissions.\")\n",
        "                           # Decide whether to skip or try to continue\n",
        "                           # results[config_key] = {\"status\": \"fail\", \"error\": f\"Directory Creation Error: {ose}\", \"accuracy\": 0, \"params\": 0}\n",
        "                           # continue # Uncomment to skip if directory fails\n",
        "\n",
        "                        print(f\"\\n--- Running Configuration: {config_key} ---\")\n",
        "                        print(f\"Output directory: {run_output_dir}\")\n",
        "\n",
        "                        # --- Initialize variables for this run's scope ---\n",
        "                        training_args_for_run = None\n",
        "                        temp_model = None\n",
        "                        lora_model_for_run = None\n",
        "                        optimizer = None\n",
        "                        lr_scheduler = None\n",
        "                        trainer_for_run = None\n",
        "                        trainable_params = 0 # Default\n",
        "\n",
        "                        try:\n",
        "                            # --- 1. Create LoRA Config ---\n",
        "                            lora_config = LoraConfig(\n",
        "                                r=r_val,\n",
        "                                lora_alpha=alpha_val,\n",
        "                                target_modules=targets,\n",
        "                                lora_dropout=dropout_val,\n",
        "                                bias=\"none\",\n",
        "                                task_type=TaskType.SEQ_CLS\n",
        "                            )\n",
        "\n",
        "                            # --- Calculate steps per epoch (needed for scheduler/logging/eval) ---\n",
        "                            if \"train\" not in tokenized_datasets:\n",
        "                                raise ValueError(\"tokenized_datasets['train'] not found.\")\n",
        "                            train_dataset_size = len(tokenized_datasets[\"train\"])\n",
        "                            if train_dataset_size == 0:\n",
        "                                raise ValueError(\"Training dataset is empty.\")\n",
        "\n",
        "                            grad_accum_steps = 1 # Assuming 1, adjust if using gradient accumulation\n",
        "                            num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "                            effective_batch_size = BATCH_SIZE * grad_accum_steps * num_gpus\n",
        "                            steps_per_epoch = max(1, train_dataset_size // effective_batch_size)\n",
        "                            total_training_steps = steps_per_epoch * epochs_val\n",
        "                            print(f\"Calculated approx steps per epoch: {steps_per_epoch}\")\n",
        "                            print(f\"Total training steps: {total_training_steps}\")\n",
        "\n",
        "                            # --- 2. Define TrainingArguments ---\n",
        "                            print(\"Defining TrainingArguments...\")\n",
        "                            training_args_for_run = TrainingArguments(\n",
        "                                output_dir=run_output_dir,\n",
        "                                num_train_epochs=epochs_val, # Use epoch value from loop\n",
        "                                learning_rate=lr,           # Use learning rate from loop\n",
        "                                per_device_train_batch_size=BATCH_SIZE,\n",
        "                                per_device_eval_batch_size=BATCH_SIZE * 2,\n",
        "                                weight_decay=0.01,\n",
        "                                # Evaluation/Saving/Logging Strategy (Example: Evaluate/Save every epoch)\n",
        "                                eval_strategy=\"epoch\",\n",
        "                                save_strategy=\"epoch\",\n",
        "                                # OR use step-based:\n",
        "                                # eval_strategy=\"steps\",\n",
        "                                # eval_steps=steps_per_epoch, # Evaluate every epoch equivalent\n",
        "                                # save_strategy=\"steps\",\n",
        "                                # save_steps=steps_per_epoch, # Save every epoch equivalent\n",
        "                                logging_strategy=\"steps\",   # Log more frequently\n",
        "                                logging_steps=max(1, steps_per_epoch // 10), # Log ~10 times per epoch\n",
        "                                load_best_model_at_end=True, # Important for finding best epoch checkpoint\n",
        "                                metric_for_best_model=\"accuracy\", # Make sure compute_metrics returns \"accuracy\"\n",
        "                                greater_is_better=True,\n",
        "                                save_total_limit=1, # Save only the best checkpoint to save space\n",
        "                                report_to=\"none\",  # Disable external reporting unless configured (like wandb)\n",
        "                                fp16=torch.cuda.is_available(), # Use mixed precision if available\n",
        "                            )\n",
        "                            print(\"Training Arguments defined successfully.\")\n",
        "\n",
        "                            # --- 3. Create Model and Apply PEFT ---\n",
        "                            print(\"Loading base model and applying PEFT...\")\n",
        "                            # Ensure id2label_map and label2id_map are defined from earlier steps\n",
        "                            if 'id2label_map' not in locals() or 'label2id_map' not in locals():\n",
        "                                 print(\"Warning: id2label_map or label2id_map not found. Using default numerical labels.\")\n",
        "                                 # Fallback or raise error depending on strictness needed\n",
        "                                 id2label_map = {i: f\"LABEL_{i}\" for i in range(NUM_LABELS)}\n",
        "                                 label2id_map = {v: k for k, v in id2label_map.items()}\n",
        "\n",
        "                            temp_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                                MODEL_CHECKPOINT,\n",
        "                                num_labels=NUM_LABELS,\n",
        "                                id2label=id2label_map,\n",
        "                                label2id=label2id_map\n",
        "                            )\n",
        "                            lora_model_for_run = get_peft_model(temp_model, lora_config)\n",
        "                            print(f\"Model created with LoRA applied. Trainable parameters:\")\n",
        "                            lora_model_for_run.print_trainable_parameters()\n",
        "\n",
        "\n",
        "                            # --- 4. Check parameters ---\n",
        "                            within_limit, trainable_params = check_params(lora_model_for_run) # Assumes function `check_params` exists\n",
        "                            if not within_limit:\n",
        "                                raise ValueError(f\"Parameter limit exceeded ({trainable_params} > limit)\") # Raise error to be caught below\n",
        "\n",
        "                            # --- 5. Define Optimizer and Scheduler ---\n",
        "                            print(\"Defining Optimizer and Scheduler...\")\n",
        "                            optimizer = torch.optim.AdamW(lora_model_for_run.parameters(), lr=lr)\n",
        "                            num_warmup_steps = int(0.1 * total_training_steps) # 10% warmup\n",
        "\n",
        "                            lr_scheduler = get_linear_schedule_with_warmup(\n",
        "                                optimizer=optimizer,\n",
        "                                num_warmup_steps=num_warmup_steps,\n",
        "                                num_training_steps=total_training_steps\n",
        "                            )\n",
        "                            print(\"Optimizer and Scheduler defined.\")\n",
        "\n",
        "                            # --- 6. Initialize Trainer ---\n",
        "                            print(\"Initializing Trainer...\")\n",
        "                             # Use validation set for evaluation during training if it exists\n",
        "                            eval_dataset = tokenized_datasets.get(\"validation\")\n",
        "                            if eval_dataset is None:\n",
        "                                print(\"Warning: No 'validation' dataset found. Using 'test' set for evaluation during training.\")\n",
        "                                eval_dataset = tokenized_datasets.get(\"test\") # Fallback to test set if no validation set\n",
        "                            if eval_dataset is None:\n",
        "                                 raise ValueError(\"Neither 'validation' nor 'test' dataset found for evaluation.\")\n",
        "\n",
        "\n",
        "                            trainer_for_run = Trainer(\n",
        "                                model=lora_model_for_run,\n",
        "                                args=training_args_for_run,\n",
        "                                train_dataset=tokenized_datasets[\"train\"],\n",
        "                                eval_dataset=eval_dataset, # Use validation (or test) for checkpoing evaluation\n",
        "                                tokenizer=tokenizer,\n",
        "                                data_collator=data_collator,\n",
        "                                compute_metrics=compute_metrics,\n",
        "                                optimizers=(optimizer, lr_scheduler)\n",
        "                            )\n",
        "                            print(\"Trainer initialized.\")\n",
        "\n",
        "                            # --- 7. Train and Evaluate ---\n",
        "                            print(f\"Starting training for {config_key}...\")\n",
        "                            train_result = trainer_for_run.train()\n",
        "                            print(f\"Training completed.\")\n",
        "\n",
        "                            # Since load_best_model_at_end=True, the trainer has loaded the best checkpoint.\n",
        "                            # Evaluate this best model on the *final test set*.\n",
        "                            print(\"Evaluating best model on the final test set...\")\n",
        "                            if \"test\" not in tokenized_datasets:\n",
        "                               raise ValueError(\"tokenized_datasets['test'] not found for final evaluation.\")\n",
        "\n",
        "                            final_eval_results = trainer_for_run.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
        "                            accuracy = final_eval_results.get(\"eval_accuracy\", 0) # Get accuracy on test set\n",
        "\n",
        "                            # Store results\n",
        "                            results[config_key] = {\"status\": \"success\", \"accuracy\": accuracy, \"params\": trainable_params}\n",
        "                            all_run_details.append({\n",
        "                                \"config\": config_key, \"r\": r_val, \"alpha\": alpha_val, \"targets\": '_'.join(targets),\n",
        "                                \"lr\": lr, \"dropout\": dropout_val, \"epochs\": epochs_val,\n",
        "                                \"params\": trainable_params, \"accuracy\": accuracy, \"output_dir\": run_output_dir\n",
        "                            })\n",
        "                            print(f\"Result for {config_key}: Final Test Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "                            # --- 8. Save Best Adapter (Optional but Recommended) ---\n",
        "                            # The best adapter is already loaded, just save it from the current model state\n",
        "                            best_adapter_save_path = os.path.join(run_output_dir, \"best_adapter\") # Simplified name\n",
        "                            trainer_for_run.model.save_pretrained(best_adapter_save_path)\n",
        "                            # Also save the tokenizer and potentially config for easier reloading\n",
        "                            tokenizer.save_pretrained(best_adapter_save_path)\n",
        "                            # trainer_for_run.model.config.save_pretrained(best_adapter_save_path) # Save base model config if needed\n",
        "\n",
        "                            print(f\"Best adapter, tokenizer, config saved to: {best_adapter_save_path}\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"ERROR during run {config_key}: {e}\")\n",
        "                            import traceback\n",
        "                            traceback.print_exc() # Print detailed traceback for debugging\n",
        "\n",
        "                            # Attempt to get parameter count even on failure, if model was created\n",
        "                            error_params = 'N/A'\n",
        "                            if 'lora_model_for_run' in locals() and lora_model_for_run is not None:\n",
        "                                try:\n",
        "                                    _, error_params = check_params(lora_model_for_run, limit=float('inf')) # Use check_params if defined\n",
        "                                except: pass\n",
        "\n",
        "                            results[config_key] = {\"status\": \"fail\", \"error\": str(e), \"accuracy\": 0, \"params\": error_params}\n",
        "                            # Optionally add failed run details to all_run_details if desired for analysis\n",
        "\n",
        "                        finally:\n",
        "                            # --- Cleanup resources for this iteration ---\n",
        "                            print(f\"Cleaning up resources for run {config_key}...\")\n",
        "                            # Delete objects in reverse order of creation (roughly)\n",
        "                            del trainer_for_run\n",
        "                            del optimizer\n",
        "                            del lr_scheduler\n",
        "                            del lora_model_for_run\n",
        "                            del temp_model\n",
        "                            del training_args_for_run\n",
        "                            # Force garbage collection and empty CUDA cache\n",
        "                            gc.collect()\n",
        "                            if torch.cuda.is_available():\n",
        "                                torch.cuda.empty_cache()\n",
        "                            print(\"-\" * 60)\n",
        "\n",
        "\n",
        "print(\"\\n--- Completed Hyperparameter Sweep ---\")\n",
        "\n",
        "# --- The rest of the script follows ---\n",
        "# Step 12: Find Best Configuration\n",
        "# Step 13: Load Best Overall Model and Save Final Adapter\n",
        "# Step 14: Display Results Summary\n",
        "# ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MMph77amv2at",
        "outputId": "52b5bd64-726e-4cec-c412-601c66bc4bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Expanded Hyperparameter Sweep ---\n",
            "\n",
            "--- Running Configuration: r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 741,124 || all params: 125,389,832 || trainable%: 0.5911\n",
            "--> Trainable Parameters: 741,124\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 09:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.255700</td>\n",
              "      <td>0.244946</td>\n",
              "      <td>0.918250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1: Final Test Accuracy = 0.9189\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 741,124 || all params: 125,389,832 || trainable%: 0.5911\n",
            "--> Trainable Parameters: 741,124\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 09:19, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.227900</td>\n",
              "      <td>0.218953</td>\n",
              "      <td>0.927000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1: Final Test Accuracy = 0.9286\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "--> Trainable Parameters: 814,852\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 10:06, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.254400</td>\n",
              "      <td>0.244737</td>\n",
              "      <td>0.918833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1: Final Test Accuracy = 0.9195\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "--> Trainable Parameters: 814,852\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 10:04, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.230200</td>\n",
              "      <td>0.221095</td>\n",
              "      <td>0.925583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1: Final Test Accuracy = 0.9267\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 741,124 || all params: 125,389,832 || trainable%: 0.5911\n",
            "--> Trainable Parameters: 741,124\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 09:19, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.257100</td>\n",
              "      <td>0.242760</td>\n",
              "      <td>0.920000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1: Final Test Accuracy = 0.9200\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 741,124 || all params: 125,389,832 || trainable%: 0.5911\n",
            "--> Trainable Parameters: 741,124\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 09:19, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.223800</td>\n",
              "      <td>0.215712</td>\n",
              "      <td>0.928750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1: Final Test Accuracy = 0.9313\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "--> Trainable Parameters: 814,852\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 10:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.251300</td>\n",
              "      <td>0.241102</td>\n",
              "      <td>0.920667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1: Final Test Accuracy = 0.9226\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "--> Trainable Parameters: 814,852\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 10:03, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.221900</td>\n",
              "      <td>0.214711</td>\n",
              "      <td>0.929583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1: Final Test Accuracy = 0.9312\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=8_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 888,580 || all params: 125,537,288 || trainable%: 0.7078\n",
            "--> Trainable Parameters: 888,580\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=8_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 09:22, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.254000</td>\n",
              "      <td>0.241685</td>\n",
              "      <td>0.917917</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=8_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1: Final Test Accuracy = 0.9203\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=8_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=8_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 888,580 || all params: 125,537,288 || trainable%: 0.7078\n",
            "--> Trainable Parameters: 888,580\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=8_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 09:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.223800</td>\n",
              "      <td>0.215295</td>\n",
              "      <td>0.927917</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=8_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1: Final Test Accuracy = 0.9289\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=8_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=8_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-30-32dffcd20c1f>\", line 149, in <cell line: 0>\n",
            "    raise ValueError(f\"Parameter limit exceeded ({trainable_params} > limit)\") # Raise error to be caught below\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ValueError: Parameter limit exceeded (1036036 > limit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 1,036,036 || all params: 125,684,744 || trainable%: 0.8243\n",
            "--> Trainable Parameters: 1,036,036\n",
            "--> WARNING: Exceeds 1,000,000 parameter limit!\n",
            "ERROR during run r=8_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1: Parameter limit exceeded (1036036 > limit)\n",
            "--> Trainable Parameters: 1,036,036\n",
            "--> Within inf parameter limit.\n",
            "Cleaning up resources for run r=8_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=8_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-30-32dffcd20c1f>\", line 149, in <cell line: 0>\n",
            "    raise ValueError(f\"Parameter limit exceeded ({trainable_params} > limit)\") # Raise error to be caught below\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ValueError: Parameter limit exceeded (1036036 > limit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 1,036,036 || all params: 125,684,744 || trainable%: 0.8243\n",
            "--> Trainable Parameters: 1,036,036\n",
            "--> WARNING: Exceeds 1,000,000 parameter limit!\n",
            "ERROR during run r=8_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1: Parameter limit exceeded (1036036 > limit)\n",
            "--> Trainable Parameters: 1,036,036\n",
            "--> Within inf parameter limit.\n",
            "Cleaning up resources for run r=8_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=8_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 888,580 || all params: 125,537,288 || trainable%: 0.7078\n",
            "--> Trainable Parameters: 888,580\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=8_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 09:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.252800</td>\n",
              "      <td>0.240653</td>\n",
              "      <td>0.919750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=8_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1: Final Test Accuracy = 0.9207\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=8_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=8_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 888,580 || all params: 125,537,288 || trainable%: 0.7078\n",
            "--> Trainable Parameters: 888,580\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=8_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 09:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.211090</td>\n",
              "      <td>0.929667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=8_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1: Final Test Accuracy = 0.9324\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1/best_adapter\n",
            "Cleaning up resources for run r=8_alpha=16_targets=query_value_lr=0.0001_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=8_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-30-32dffcd20c1f>\", line 149, in <cell line: 0>\n",
            "    raise ValueError(f\"Parameter limit exceeded ({trainable_params} > limit)\") # Raise error to be caught below\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ValueError: Parameter limit exceeded (1036036 > limit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 1,036,036 || all params: 125,684,744 || trainable%: 0.8243\n",
            "--> Trainable Parameters: 1,036,036\n",
            "--> WARNING: Exceeds 1,000,000 parameter limit!\n",
            "ERROR during run r=8_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1: Parameter limit exceeded (1036036 > limit)\n",
            "--> Trainable Parameters: 1,036,036\n",
            "--> Within inf parameter limit.\n",
            "Cleaning up resources for run r=8_alpha=16_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=8_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=8_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 6750\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-30-32dffcd20c1f>\", line 149, in <cell line: 0>\n",
            "    raise ValueError(f\"Parameter limit exceeded ({trainable_params} > limit)\") # Raise error to be caught below\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ValueError: Parameter limit exceeded (1036036 > limit)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 1,036,036 || all params: 125,684,744 || trainable%: 0.8243\n",
            "--> Trainable Parameters: 1,036,036\n",
            "--> WARNING: Exceeds 1,000,000 parameter limit!\n",
            "ERROR during run r=8_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1: Parameter limit exceeded (1036036 > limit)\n",
            "--> Trainable Parameters: 1,036,036\n",
            "--> Within inf parameter limit.\n",
            "Cleaning up resources for run r=8_alpha=16_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=1...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=3 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=3\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 20250\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 741,124 || all params: 125,389,832 || trainable%: 0.5911\n",
            "--> Trainable Parameters: 741,124\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=3...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20250' max='20250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20250/20250 28:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.256700</td>\n",
              "      <td>0.241213</td>\n",
              "      <td>0.919333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.220300</td>\n",
              "      <td>0.221062</td>\n",
              "      <td>0.926083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.206000</td>\n",
              "      <td>0.211672</td>\n",
              "      <td>0.931167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=3: Final Test Accuracy = 0.9305\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=3/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=8_targets=query_value_lr=5e-05_dropout=0.1_epochs=3...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=3 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=3\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 20250\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 741,124 || all params: 125,389,832 || trainable%: 0.5911\n",
            "--> Trainable Parameters: 741,124\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=3...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20250' max='20250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20250/20250 28:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.227700</td>\n",
              "      <td>0.215136</td>\n",
              "      <td>0.926750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.196800</td>\n",
              "      <td>0.204530</td>\n",
              "      <td>0.931667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.192862</td>\n",
              "      <td>0.937167</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result for r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=3: Final Test Accuracy = 0.9363\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=3/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=8_targets=query_value_lr=0.0001_dropout=0.1_epochs=3...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=3 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=3\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 20250\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "--> Trainable Parameters: 814,852\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=3...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16680' max='20250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16680/20250 24:37 < 05:16, 11.28 it/s, Epoch 2.47/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.251100</td>\n",
              "      <td>0.242255</td>\n",
              "      <td>0.917833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.215600</td>\n",
              "      <td>0.215232</td>\n",
              "      <td>0.928417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20250' max='20250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20250/20250 30:08, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.251100</td>\n",
              "      <td>0.242255</td>\n",
              "      <td>0.917833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.215600</td>\n",
              "      <td>0.215232</td>\n",
              "      <td>0.928417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.202800</td>\n",
              "      <td>0.208353</td>\n",
              "      <td>0.931750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=3: Final Test Accuracy = 0.9330\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=3/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=8_targets=query_key_value_lr=5e-05_dropout=0.1_epochs=3...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=3 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=3\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 20250\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "--> Trainable Parameters: 814,852\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=3...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20250' max='20250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20250/20250 30:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.226000</td>\n",
              "      <td>0.218550</td>\n",
              "      <td>0.924417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.196600</td>\n",
              "      <td>0.194012</td>\n",
              "      <td>0.934417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.182100</td>\n",
              "      <td>0.191426</td>\n",
              "      <td>0.937250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed.\n",
            "Evaluating best model on the final test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [238/238 00:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=3: Final Test Accuracy = 0.9387\n",
            "Best adapter, tokenizer, config saved to: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=3/best_adapter\n",
            "Cleaning up resources for run r=4_alpha=8_targets=query_key_value_lr=0.0001_dropout=0.1_epochs=3...\n",
            "------------------------------------------------------------\n",
            "\n",
            "--- Running Configuration: r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=3 ---\n",
            "Output directory: /content/drive/MyDrive/Colab_Checkpoints/AGNEWS_LORA_Project/r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=3\n",
            "Calculated approx steps per epoch: 6750\n",
            "Total training steps: 20250\n",
            "Defining TrainingArguments...\n",
            "TrainingArguments defined successfully.\n",
            "Loading base model and applying PEFT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-30-32dffcd20c1f>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_for_run = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created with LoRA applied. Trainable parameters:\n",
            "trainable params: 741,124 || all params: 125,389,832 || trainable%: 0.5911\n",
            "--> Trainable Parameters: 741,124\n",
            "--> Within 1,000,000 parameter limit.\n",
            "Defining Optimizer and Scheduler...\n",
            "Optimizer and Scheduler defined.\n",
            "Initializing Trainer...\n",
            "Trainer initialized.\n",
            "Starting training for r=4_alpha=16_targets=query_value_lr=5e-05_dropout=0.1_epochs=3...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8584' max='20250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8584/20250 11:47 < 16:02, 12.12 it/s, Epoch 1.27/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.252300</td>\n",
              "      <td>0.239072</td>\n",
              "      <td>0.919417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Find Best Configuration After Sweep\n",
        "print(\"\\n--- Sweep Finished ---\")\n",
        "\n",
        "successful_runs = {k: v for k, v in results.items() if v.get(\"status\") == \"success\"} # Use .get() for safety\n",
        "\n",
        "if successful_runs:\n",
        "    # Find best config based on accuracy\n",
        "    best_config_key = max(successful_runs, key=lambda k: successful_runs[k][\"accuracy\"])\n",
        "    best_accuracy = successful_runs[best_config_key][\"accuracy\"]\n",
        "    best_params = successful_runs[best_config_key][\"params\"]\n",
        "    # Find the corresponding details from all_run_details\n",
        "    best_run_details = next((run for run in all_run_details if run[\"config\"] == best_config_key), None) # Added default None\n",
        "\n",
        "    print(f\"Best configuration found: {best_config_key}\")\n",
        "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"Trainable Parameters: {best_params:,}\")\n",
        "\n",
        "    # Step 13: Load the Overall Best Model and Save Final Adapter\n",
        "    if best_run_details and DRIVE_MOUNTED:\n",
        "        best_run_output_dir = best_run_details[\"output_dir\"]\n",
        "        # Path to the adapter saved at the end of the best run\n",
        "        best_adapter_path = os.path.join(best_run_output_dir, \"best_adapter_for_run\") # Corrected path variable name\n",
        "\n",
        "        if os.path.exists(best_adapter_path):\n",
        "            print(f\"\\nLoading best adapter from: {best_adapter_path}\")\n",
        "            try:\n",
        "                # Load the base model first\n",
        "                base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    MODEL_CHECKPOINT, num_labels=NUM_LABELS\n",
        "                )\n",
        "                # Load the LoRA adapter onto the base model\n",
        "                best_lora_model = PeftModel.from_pretrained(base_model, best_adapter_path) # Correct variable name\n",
        "                best_lora_model.to('cuda' if torch.cuda.is_available() else 'cpu') # Move model to device\n",
        "                print(\"Successfully loaded best LoRA model.\")\n",
        "\n",
        "                # Define path for the final overall best adapter\n",
        "                final_best_adapter_dir = os.path.join(DRIVE_BASE_DIR, \"final_best_adapter\") # Correct variable name\n",
        "                os.makedirs(final_best_adapter_dir, exist_ok=True)\n",
        "\n",
        "                # Save the final best adapter weights\n",
        "                best_lora_model.save_pretrained(final_best_adapter_dir)\n",
        "                print(f\"Final best adapter saved to: {final_best_adapter_dir}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading or saving the final best model: {e}\")\n",
        "        else:\n",
        "             print(f\"Could not find best adapter files at expected location: {best_adapter_path}\") # Correct variable name\n",
        "             print(\"Skipping final model loading and saving.\")\n",
        "\n",
        "    elif not DRIVE_MOUNTED:\n",
        "        print(\"\\nSkipping final model loading and saving as Google Drive was not mounted.\")\n",
        "    # Added case where best_run_details might be None even if successful_runs has items (shouldn't happen with current logic but safe)\n",
        "    elif not best_run_details:\n",
        "         print(\"\\nCould not find details for the best run. Skipping final model loading.\")\n",
        "\n",
        "else:\n",
        "    print(\"No successful runs completed in the hyperparameter sweep.\")\n",
        "\n",
        "\n",
        "# Step 14: Display Results Summary (Example)\n",
        "print(\"\\n--- Run Summary ---\")\n",
        "if all_run_details:\n",
        "    # Ensure 'accuracy' and 'params' columns exist before sorting/displaying\n",
        "    summary_df = pd.DataFrame(all_run_details)\n",
        "    display_cols = ['config', 'params', 'accuracy']\n",
        "    # Filter columns to only those present in the DataFrame\n",
        "    display_cols = [col for col in display_cols if col in summary_df.columns]\n",
        "    if 'accuracy' in display_cols:\n",
        "      print(summary_df[display_cols].sort_values(by='accuracy', ascending=False))\n",
        "    elif display_cols: # If accuracy column is missing, print available cols\n",
        "        print(summary_df[display_cols])\n",
        "    else:\n",
        "        print(\"No relevant columns found in run details.\")\n",
        "\n",
        "else:\n",
        "    print(\"No run details available.\")\n",
        "\n",
        "\n",
        "print(\"\\nCode execution finished.\")"
      ],
      "metadata": {
        "id": "wLq2E_CjPoXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KAGGLE SUBMISSION INFERENCE (Adapted for Google Drive & Corrected)\n",
        "import pandas as pd\n",
        "import pickle # Import the pickle library\n",
        "from datasets import Dataset # Ensure Dataset is imported\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\n",
        "# Ensure 'PeftModel', 'AutoModelForSequenceClassification' were imported earlier\n",
        "# Ensure 'best_lora_model' variable holds your trained PEFT model loaded onto the base model.\n",
        "# Ensure 'tokenizer' and 'data_collator' are defined from training or reloaded\n",
        "# Ensure BATCH_SIZE and MAX_LENGTH constants are defined from training\n",
        "\n",
        "print(\"--- KAGGLE SUBMISSION INFERENCE START ---\")\n",
        "\n",
        "# --- Mount Google Drive if not already mounted ---\n",
        "from google.colab import drive\n",
        "try:\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "      print(\"Mounting Google Drive...\")\n",
        "      drive.mount('/content/drive')\n",
        "      print(\"Google Drive mounted.\")\n",
        "    else:\n",
        "      print(\"Google Drive already mounted.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    raise SystemExit(\"Google Drive mounting failed, cannot access test data.\")\n",
        "\n",
        "# --- Load the Unlabelled Test Data from PKL file ---\n",
        "test_pkl_path = \"/content/drive/MyDrive/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\"\n",
        "print(f\"Attempting to load test data from: {test_pkl_path}\")\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(test_pkl_path):\n",
        "        raise FileNotFoundError(f\"File not found at the specified Google Drive path: {test_pkl_path}\")\n",
        "\n",
        "    # Load the data (which we know results in a Dataset object)\n",
        "    test_dataset_hf = pd.read_pickle(test_pkl_path) # Load directly into the final variable name\n",
        "    print(f\"Loaded test data. Object type: {type(test_dataset_hf)}\")\n",
        "\n",
        "    # Verify it's a Dataset and has the 'text' feature\n",
        "    if not isinstance(test_dataset_hf, Dataset):\n",
        "         raise TypeError(f\"Loaded data is not a Dataset object, but {type(test_dataset_hf)}\")\n",
        "    if 'text' not in test_dataset_hf.features:\n",
        "        raise ValueError(f\"Loaded dataset does not contain the required 'text' feature. Features found: {test_dataset_hf.features}\")\n",
        "\n",
        "    print(\"\\nTest Dataset Info:\")\n",
        "    print(test_dataset_hf)\n",
        "    print(\"\\nTest Dataset Features:\")\n",
        "    print(test_dataset_hf.features)\n",
        "    print(\"\\nFirst 5 examples:\")\n",
        "    print(test_dataset_hf[0:5]) # Correct way to view head\n",
        "\n",
        "except FileNotFoundError as fnf_e:\n",
        "    print(f\"Error: {fnf_e}\")\n",
        "    print(\"Please double-check the file path and ensure Google Drive is mounted correctly.\")\n",
        "    raise SystemExit(\"Failed to load test data due to FileNotFoundError.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during data loading: {e}\")\n",
        "    raise SystemExit(\"Failed to load or validate test data.\")\n",
        "\n",
        "# --- Set Text Column Name ---\n",
        "text_column_name = \"text\" # Confirmed from features\n",
        "\n",
        "# --- Preprocess/Tokenize the Test Data ---\n",
        "# Define tokenization function using the correct text column name\n",
        "def tokenize_for_inference(examples):\n",
        "    return tokenizer(examples[text_column_name], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
        "\n",
        "print(f\"\\nTokenizing test data using column: '{text_column_name}'...\")\n",
        "# Apply map directly to the loaded dataset\n",
        "tokenized_test_kaggle = test_dataset_hf.map(tokenize_for_inference, batched=True, remove_columns=[text_column_name]) # Remove original text column\n",
        "\n",
        "# Set format for PyTorch\n",
        "tokenized_test_kaggle.set_format(\"torch\")\n",
        "print(f\"Test data tokenized. Columns kept: {tokenized_test_kaggle.column_names}\") # Show remaining columns\n",
        "\n",
        "# --- Generate Predictions ---\n",
        "print(\"\\nGenerating predictions on Kaggle test set...\")\n",
        "# Ensure the model is on the correct device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Make sure best_lora_model is loaded and moved to device\n",
        "if 'best_lora_model' not in locals():\n",
        "     raise NameError(\"Variable 'best_lora_model' not defined. Load the model before inference.\")\n",
        "best_lora_model.to(device)\n",
        "best_lora_model.eval() # Set model to evaluation mode\n",
        "\n",
        "# Create a temporary Trainer for prediction\n",
        "temp_training_args = TrainingArguments(\n",
        "    output_dir=\"/content/temp_preds\", # Use a writable path in Colab\n",
        "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        "    logging_dir=None, # Disable logging for prediction\n",
        ")\n",
        "\n",
        "# Ensure data_collator is defined\n",
        "if 'data_collator' not in locals():\n",
        "    print(\"Warning: 'data_collator' not defined. Defining default DataCollatorWithPadding.\")\n",
        "    if 'tokenizer' not in locals():\n",
        "         raise NameError(\"Cannot define data_collator because 'tokenizer' is not defined.\")\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "pred_trainer = Trainer(\n",
        "    model=best_lora_model,\n",
        "    args=temp_training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Running prediction...\")\n",
        "test_predictions = pred_trainer.predict(tokenized_test_kaggle)\n",
        "predicted_logits = test_predictions.predictions\n",
        "\n",
        "# --- Generate Predicted Label IDs (0-3) --- Corrected mapping\n",
        "predicted_label_ids = np.argmax(predicted_logits, axis=-1) # Direct 0-3 labels\n",
        "\n",
        "# --- Generate Sequential IDs ---\n",
        "num_predictions = len(predicted_label_ids)\n",
        "print(f\"Generated {num_predictions} predictions.\")\n",
        "# Assuming 0-based IDs (0 to num_predictions-1) as per common practice\n",
        "generated_ids = range(num_predictions)\n",
        "# If 1-based IDs are needed (1 to num_predictions), use:\n",
        "# generated_ids = range(1, num_predictions + 1)\n",
        "\n",
        "# --- Format for Submission File ---\n",
        "print(\"\\nFormatting predictions for submission...\")\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ID\": generated_ids,          # Use the generated IDs\n",
        "    \"Label\": predicted_label_ids  # Use the direct 0-3 label IDs\n",
        "})\n",
        "\n",
        "# --- Save Submission File ---\n",
        "# Define a writable path, e.g., in your Drive\n",
        "submission_path = \"/content/drive/MyDrive/kaggle/output/submission.csv\"\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = os.path.dirname(submission_path)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "submission_df.to_csv(submission_path, index=False)\n",
        "\n",
        "print(f\"\\nSubmission file successfully saved to: {submission_path}\")\n",
        "print(\"\\nSubmission file head:\")\n",
        "print(submission_df.head())\n",
        "print(f\"\\nTotal predictions generated: {len(submission_df)}\")\n",
        "\n",
        "print(\"\\n--- KAGGLE SUBMISSION INFERENCE END ---\")"
      ],
      "metadata": {
        "id": "LmSEzj3_SrrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qf0fsn0A6EmQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}